{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03whilqqIkqi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define labels\n",
        "labels = [\"squamous.cell.carcinoma\", \"normal\", \"adenocarcinoma\", \"large.cell.carcinoma\"]\n",
        "\n",
        "# Initialize an empty list to store the counts\n",
        "test_list = []\n",
        "\n",
        "# Iterate through the labels and retrieve the counts from the test_set dictionary\n",
        "for label in labels:\n",
        "    count = test_set.get(label, 0)\n",
        "    # Get the count for the label or 0 if label not found\n",
        "    test_list.append(count)\n",
        "\n",
        "# Convert test_list to numpy array\n",
        "test_list = np.array(test_list)\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "test_bar = ax.bar(x, test_list, width, label=\"Test\")\n",
        "\n",
        "ax.set_ylabel('# of Images')\n",
        "ax.set_title('Chest CT Scan Dataset')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=15)  # Set labels with rotation\n",
        "ax.legend()\n",
        "\n",
        "# Add labels to the bars\n",
        "for bar in test_bar:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate('{}'.format(height),\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # 3 points vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "# Create an instance of ImageDataGenerator for the test dataset\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Use flow_from_directory to generate a DirectoryIterator for the test dataset\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(dataset_dir, \"test\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,  # set batch size to 1 to count files individually\n",
        "    class_mode='categorical',  # set class_mode to categorical\n",
        "    shuffle=False\n",
        ")\n",
        "# Similarly, create DirectoryIterators for the train and validation datasets\n",
        "train_generator = test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(dataset_dir, \"train\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_generator = test_datagen.flow_from_directory(\n",
        "    directory=os.path.join(dataset_dir, \"valid\"),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Get the number of samples for each class in the test dataset\n",
        "test_class_counts = test_generator.classes.sum(axis=0)\n",
        "\n",
        "# Get the number of samples for each class in the train and validation datasets\n",
        "train_class_counts = train_generator.classes.sum(axis=0)\n",
        "val_class_counts = val_generator.classes.sum(axis=0)\n",
        "\n",
        "print(\"Test dataset class counts:\", test_class_counts)\n",
        "print(\"Train dataset class counts:\", train_class_counts)\n",
        "print(\"Validation dataset class counts:\", val_class_counts)\n",
        "# Function to get the number of files for each class in a specified directory\n",
        "def get_file_count(directory):\n",
        "    counts = []\n",
        "    for label in class_labels:\n",
        "        class_dir = os.path.join(directory, label)\n",
        "        if os.path.exists(class_dir):\n",
        "            counts.append(len(os.listdir(class_dir)))\n",
        "        else:\n",
        "            counts.append(0)\n",
        "    return counts\n",
        "\n",
        "# Get the number of files for each class in the test dataset\n",
        "test_counts = get_file_count(os.path.join(dataset_dir, \"test\"))\n",
        "\n",
        "# Get the number of files for each class in the train dataset\n",
        "train_counts = get_file_count(os.path.join(dataset_dir, \"train\"))\n",
        "\n",
        "# Get the number of files for each class in the validation dataset\n",
        "val_counts = get_file_count(os.path.join(dataset_dir, \"valid\"))\n",
        "# Assuming you have already defined test_generator and test_class_names\n",
        "test_class_names = [\"squamous.cell.carcinoma\", \"normal\", \"adenocarcinoma\", \"large.cell.carcinoma\"]\n",
        "\n",
        "# Assuming you have evaluated the test set and obtained predictions\n",
        "# Let's create some sample predictions for demonstration\n",
        "y_true = np.random.randint(0, 4, size=(72,))\n",
        "y_pred = np.random.randint(0, 4, size=(72,))\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(12):\n",
        "    image, label = next(test_generator)\n",
        "    ax = plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(image[0].astype(\"uint8\"))  # Assuming batch size is 1\n",
        "    plt.title(f\"True: {test_class_names[y_true[i]]}, Pred: {test_class_names[y_pred[i]]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "# Initialize empty lists to store data\n",
        "X, Y = [], []\n",
        "\n",
        "# Obtain data\n",
        "obtain_data(X, Y, train_dir)\n",
        "obtain_data(X, Y, test_dir)\n",
        "obtain_data(X, Y, val_dir)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# Encode labels\n",
        "Y_encoded = np.array([np.argmax(np.array(unique_classes) == label) for label in Y])\n",
        "\n",
        "# Split data into train, test, and validation sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.2, random_state=42)\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n",
        "# Print the shape of the data\n",
        "print(\"Train Data:\")\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"\\nTest Data:\")\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Y_test shape:\", Y_test.shape)\n",
        "print(\"\\nValidation Data:\")\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"Y_val shape:\", Y_val.shape)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(np.unique(Y)), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    validation_data=(X_val, Y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)  # create a batch\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    predicted_class = test_class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict labels for the test set\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(Y), yticklabels=np.unique(Y))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ]
}